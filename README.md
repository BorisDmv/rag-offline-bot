# Offline RAG bot

This project implements an offline Retrieval-Augmented Generation (RAG) chatbot using Python. It allows you to query a set of documents and receive answers generated by a local Large Language Model (LLM), without relying on external APIs.

## Features

* **Offline Operation:** The chatbot works entirely offline, using a local LLM (Mistral) and a local vector database (FAISS).
* **Retrieval-Augmented Generation:** It retrieves relevant information from a set of documents to provide more accurate and context-aware answers.
* **Sentence Embeddings:** It uses Sentence Transformers to generate embeddings for the documents and the user queries.
* **FAISS Index:** It uses FAISS for efficient similarity search to find the most relevant documents.
* **GGUF Model:** It uses a GGUF format model for the LLM, loaded with `llama-cpp-python`.

## Project Structure

* `rag-bot.py`: The main Python script that implements the RAG chatbot.
* `rag_text_chunks.csv`: (Excluded from Git) Contains the text chunks extracted from the documents used for retrieval.
* `.gitignore`: Specifies files and directories that Git should ignore.

## Dependencies

* Python 3.x
* pandas
* numpy
* faiss-cpu
* sentence-transformers
* llama-cpp-python
* dotenv

## Setup

1.  **Install dependencies:**

    ```bash
    pip install -r requirements.txt
    ```

2.  **Download LLM Model:**

    * Download a GGUF format model (e.g., Mistral) from a repository like [TheBloke/Mistral-7B-Instruct-v0.1-GGUF](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF) on Hugging Face.  Place the downloaded model file in the location specified by the `GGUF_MODEL_PATH` environment variable in a `.env` file. For example:

        ```
        GGUF_MODEL_PATH=/path/to/your/model.gguf
        ```

3.  **Prepare Document Chunks:**

    * The current implementation expects the document chunks to be in a CSV file named `rag_text_chunks.csv` with a column named "content". You'll need to pre-process your documents into suitable chunks and save them in this format. This process is outside the scope of this README.

4.  **Run the chatbot:**

    ```bash
    python rag-bot.py
    ```

## Usage

1.  Run the `rag-bot.py` script.
2.  The chatbot will start and prompt you to enter your queries.
3.  Type your query and press Enter.
4.  The chatbot will retrieve relevant information from the documents, generate an answer using the LLM, and display the response.
5.  Type "quit" to exit the chatbot.

## .gitignore Contents

The `.gitignore` file is configured to exclude the following files and directories from Git version control:

* Python bytecode files (`*.pyc`, `__pycache__/`)
* Virtual environment directories (`.venv/`, `venv/`, `env/`, `envs/`)
* OS-generated files (`.DS_Store`, `Thumbs.db`)
* Log files (`*.log`)
* Data files (`*.csv`, `*.parquet`, `*.json`, `rag_text_chunks.csv`)
* Temporary files (`temp/`, `tmp/`, `*.tmp`)
* IDE-specific files (`.idea/`, `.vscode/`, `*.swp`, `*.swo`, `*.sublime-project`, `*.sublime-workspace`)
* Dependency cache directories (`/node_modules`, `/build`, `/dist/`, `/target`, `/out`)
* Sentence Transformers cache directory
* Environment variable files (`.env`, `*config.yaml`, `*config.yml`, `secrets.json`, `private_key.pem`)
* Text files except for `important.txt`

## Note

* The `rag_text_chunks.csv` file is excluded from Git as it may contain large amounts of data. You will need to generate this file yourself, containing the pre-processed text chunks from your knowledge base.
* Ensure that the `GGUF_MODEL_PATH` in your `.env` file points to the correct path of your downloaded GGUF model.
* This chatbot is designed for offline use. No data is sent to external servers.