# Offline RAG Bot

This project implements an offline Retrieval-Augmented Generation (RAG) chatbot using Python. It allows you to query a set of documents and receive answers generated by a local Large Language Model (LLM), without relying on external APIs.

## Features

* **Offline Operation:** The chatbot works entirely offline, using a local LLM (Mistral) and a local vector database (FAISS).
* **Retrieval-Augmented Generation:** It retrieves relevant information from a set of documents to provide more accurate and context-aware answers.
* **Sentence Embeddings:** It uses Sentence Transformers to generate embeddings for the documents and the user queries.
* **FAISS Index:** It uses FAISS for efficient similarity search to find the most relevant documents.
* **GGUF Model:** It uses a GGUF format model for the LLM, loaded with `llama-cpp-python`.
* **Gunicorn Support:** Production-ready serving using Gunicorn and HTTPS support.
* **Dockerized:** Easily deployable via Docker.

## Project Structure

* `rag-bot.py`: The main Python script that implements the RAG chatbot.
* `rag_text_chunks.csv`: (Excluded from Git) Contains the text chunks extracted from the documents used for retrieval.
* `.gitignore`: Specifies files and directories that Git should ignore.
* `Dockerfile`: Production Dockerfile using Gunicorn and Python 3.9.

## Dependencies

* Python 3.x
* pandas
* numpy
* faiss-cpu
* sentence-transformers
* llama-cpp-python
* python-dotenv
* flask
* gunicorn

## Setup

### Virtual Environment

```bash
python -m venv venv
```

Activate the virtual environment:

On macOS/Linux:

```bash
source venv/bin/activate
```

On Windows:

```bash
venv\Scripts\activate
```

### Install dependencies

```bash
pip install -r requirements.txt
```

### Download LLM Model

* Download a GGUF format model (e.g., Mistral) from a repository like [TheBloke/Mistral-7B-Instruct-v0.1-GGUF](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF) on Hugging Face.
* Place the downloaded model in the location specified by the `GGUF_MODEL_PATH` in a `.env` file.

```
GGUF_MODEL_PATH=/path/to/your/model.gguf
```

### Prepare Document Chunks

* Save your document chunks into `rag_text_chunks.csv` with a column named `"content"`.

### Optional (HTTPS for Local Development)

```bash
openssl req -x509 -newkey rsa:4096 -keyout key.pem -out cert.pem -days 365 -nodes
```

### Run the chatbot

```bash
python rag-bot.py
```

## Docker Usage

Build the Docker image:

```bash
docker build -t rag-bot .
```

Run the container:

```bash
docker run -p 9090:9090 --env-file .env rag-bot
```

This uses `gunicorn` for serving via:

```
http://localhost:9090
```

## Gunicorn Configuration

Your Dockerfile uses:

```dockerfile
CMD ["gunicorn", "--bind", "0.0.0.0:9090", "app:app"]
```

You can also pass a Gunicorn config file using `-c gunicorn_config.py` if needed.

## Usage

1. Run the chatbot.
2. Enter your query in the console or via a web interface if served.
3. The bot will return an answer using context from your documents.
4. Type "quit" to exit the CLI mode.

## .gitignore Contents

The `.gitignore` file is configured to exclude:

* Python bytecode files (`*.pyc`, `__pycache__/`)
* Virtual environments (`.venv/`, `venv/`, etc.)
* OS files (`.DS_Store`, `Thumbs.db`)
* Logs and temp files
* Data files like `rag_text_chunks.csv`
* IDE/project configs (`.idea/`, `.vscode/`)
* Secrets and `.env` files

## Note

* You must generate `rag_text_chunks.csv` yourself.
* The chatbot runs fully offline.
* Make sure `GGUF_MODEL_PATH` is set correctly in your `.env` file.